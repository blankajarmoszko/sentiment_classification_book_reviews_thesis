{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa05f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0c83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe5a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"review_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6297449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>num_likes</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Stolen Heir</td>\n",
       "      <td>Emily May</td>\n",
       "      <td>2,031</td>\n",
       "      <td>310k</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Not quite The Cruel Prince but I enjoyed this ...</td>\n",
       "      <td>1,206</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Stolen Heir</td>\n",
       "      <td>jessica</td>\n",
       "      <td>2,563</td>\n",
       "      <td>42.2k</td>\n",
       "      <td>4.0</td>\n",
       "      <td>hmm. i didnt obsess over this one like i did w...</td>\n",
       "      <td>654</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Stolen Heir</td>\n",
       "      <td>katia</td>\n",
       "      <td>306</td>\n",
       "      <td>526</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5 ‚òÖoak was a disappointment, but suren!! i l...</td>\n",
       "      <td>1,233</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Stolen Heir</td>\n",
       "      <td>cor</td>\n",
       "      <td>269</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OMG IM SHAKING IM SO EXCITED IM GOING TO CRY I...</td>\n",
       "      <td>610</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Stolen Heir</td>\n",
       "      <td>ale (semi hiatus) ‚Äß ‚ÇäÀö‡≠® ‚ô° ‡≠ß ‚ÇäÀö</td>\n",
       "      <td>438</td>\n",
       "      <td>2,598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WE HAVE A COVER, OMG, WE HAVE A COVER!!!!I NEE...</td>\n",
       "      <td>746</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                   reviewer_name num_reviews num_followers  \\\n",
       "0  The Stolen Heir                       Emily May       2,031          310k   \n",
       "1  The Stolen Heir                         jessica       2,563         42.2k   \n",
       "2  The Stolen Heir                           katia         306           526   \n",
       "3  The Stolen Heir                             cor         269            74   \n",
       "4  The Stolen Heir  ale (semi hiatus) ‚Äß ‚ÇäÀö‡≠® ‚ô° ‡≠ß ‚ÇäÀö         438         2,598   \n",
       "\n",
       "   star_rating                                        review_text num_likes  \\\n",
       "0          4.0  Not quite The Cruel Prince but I enjoyed this ...     1,206   \n",
       "1          4.0  hmm. i didnt obsess over this one like i did w...       654   \n",
       "2          3.0  3.5 ‚òÖoak was a disappointment, but suren!! i l...     1,233   \n",
       "3          NaN  OMG IM SHAKING IM SO EXCITED IM GOING TO CRY I...       610   \n",
       "4          NaN  WE HAVE A COVER, OMG, WE HAVE A COVER!!!!I NEE...       746   \n",
       "\n",
       "   num_comments  \n",
       "0           3.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6199f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(subset=[\"star_rating\",\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40fdad9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8399, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8781ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7850, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c3f9f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows per star rating:\n",
      "1    3271\n",
      "2    2733\n",
      "0    1209\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to map stars to sentiment\n",
    "def map_sentiment(stars_received):\n",
    "    if stars_received <= 2:\n",
    "        return 0\n",
    "    elif stars_received <= 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "# Mapping stars to sentiment into three categories\n",
    "df_cleaned['sentiment'] = [ map_sentiment(x) for x in df_cleaned['star_rating']]\n",
    "print(\"Number of rows per star rating:\")\n",
    "print(df_cleaned['sentiment'].value_counts())\n",
    "\n",
    "# Plotting the sentiment distribution\n",
    "plt.figure()\n",
    "pd.value_counts(df_cleaned['sentiment']).plot.bar(title=\"Sentiment distribution in df\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"No. of rows in df\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c714042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Return the list of words separated by commas\n",
    "    return ','.join(tokens)\n",
    "\n",
    "# Apply the preprocess_text function to the \"review_text\" column\n",
    "df_cleaned[\"cleaned_text\"] = df_cleaned[\"review_text\"].apply(preprocess_text)\n",
    "\n",
    "# Drop rows with NaN values in the \"cleaned_text\" column\n",
    "df_cleaned = df_cleaned.dropna(subset=[\"cleaned_text\"])\n",
    "\n",
    "# Check for foreign characters using a regular expression\n",
    "df_cleaned = df_cleaned[df_cleaned[\"cleaned_text\"].str.match(r'^[a-zA-Z\\s,]+$')]\n",
    "\n",
    "# Reset index after removing rows\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f18d92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    quit,cruel,princ,enjoy,loti,love,back,world,lo...\n",
       "1    hmm,didnt,obsess,one,like,origin,trilogywhil,e...\n",
       "2    oak,disappoint,suren,love,much,cant,pretend,do...\n",
       "3                everi,singl,mention,oak,hoov,jumpscar\n",
       "4                                                 star\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[\"cleaned_text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17b06fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>num_likes</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Stolen Heir</td>\n",
       "      <td>Emily May</td>\n",
       "      <td>2,031</td>\n",
       "      <td>310k</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Not quite The Cruel Prince but I enjoyed this ...</td>\n",
       "      <td>1,206</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>quit,cruel,princ,enjoy,loti,love,back,world,lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Stolen Heir</td>\n",
       "      <td>jessica</td>\n",
       "      <td>2,563</td>\n",
       "      <td>42.2k</td>\n",
       "      <td>4.0</td>\n",
       "      <td>hmm. i didnt obsess over this one like i did w...</td>\n",
       "      <td>654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>hmm,didnt,obsess,one,like,origin,trilogywhil,e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Stolen Heir</td>\n",
       "      <td>katia</td>\n",
       "      <td>306</td>\n",
       "      <td>526</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5 ‚òÖoak was a disappointment, but suren!! i l...</td>\n",
       "      <td>1,233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>oak,disappoint,suren,love,much,cant,pretend,do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Stolen Heir</td>\n",
       "      <td>madeline</td>\n",
       "      <td>111</td>\n",
       "      <td>18</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Every single mention of Oak‚Äôs hooves was a jum...</td>\n",
       "      <td>1,707</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>everi,singl,mention,oak,hoov,jumpscar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Stolen Heir</td>\n",
       "      <td>Haley pham</td>\n",
       "      <td>81</td>\n",
       "      <td>143k</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5 stars ‚≠êÔ∏èüßöüèªü•π</td>\n",
       "      <td>1,432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>star</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title reviewer_name num_reviews num_followers  star_rating  \\\n",
       "0  The Stolen Heir     Emily May       2,031          310k          4.0   \n",
       "1  The Stolen Heir       jessica       2,563         42.2k          4.0   \n",
       "2  The Stolen Heir         katia         306           526          3.0   \n",
       "3  The Stolen Heir     madeline          111            18          3.0   \n",
       "4  The Stolen Heir    Haley pham          81          143k          4.0   \n",
       "\n",
       "                                         review_text num_likes  num_comments  \\\n",
       "0  Not quite The Cruel Prince but I enjoyed this ...     1,206           3.0   \n",
       "1  hmm. i didnt obsess over this one like i did w...       654           0.0   \n",
       "2  3.5 ‚òÖoak was a disappointment, but suren!! i l...     1,233           0.0   \n",
       "3  Every single mention of Oak‚Äôs hooves was a jum...     1,707           4.0   \n",
       "4                                    4.5 stars ‚≠êÔ∏èüßöüèªü•π     1,432           0.0   \n",
       "\n",
       "   sentiment                                       cleaned_text  \n",
       "0          1  quit,cruel,princ,enjoy,loti,love,back,world,lo...  \n",
       "1          1  hmm,didnt,obsess,one,like,origin,trilogywhil,e...  \n",
       "2          1  oak,disappoint,suren,love,much,cant,pretend,do...  \n",
       "3          1              everi,singl,mention,oak,hoov,jumpscar  \n",
       "4          1                                               star  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "772ad43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89b6e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have a DataFrame df_cleaned with columns 'cleaned_text' and 'sentiment'\n",
    "# Replace 'your_dataset.csv' with the actual file or provide your own dataset\n",
    "# df_cleaned = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(df_cleaned['cleaned_text'], df_cleaned['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_train_data = [text.split(',') for text in train_data]\n",
    "tokenized_test_data = [text.split(',') for text in test_data]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_train_data, vector_size=300, window=5, min_count=1, workers=4, sg=0)\n",
    "\n",
    "# Create average word vectors for each document\n",
    "def average_word_vectors(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "\n",
    "    for word in words:\n",
    "        if word in model.wv.index_to_key:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "# Create document vectors for the training set\n",
    "X_train = np.vstack([average_word_vectors(words, word2vec_model, 300) for words in tokenized_train_data])\n",
    "\n",
    "# Create document vectors for the test set\n",
    "X_test = np.vstack([average_word_vectors(words, word2vec_model, 300) for words in tokenized_test_data])\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3def6534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.30      0.40       259\n",
      "           0       0.56      0.67      0.61       658\n",
      "           1       0.60      0.61      0.61       526\n",
      "\n",
      "    accuracy                           0.58      1443\n",
      "   macro avg       0.59      0.53      0.54      1443\n",
      "weighted avg       0.59      0.58      0.57      1443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and train the logistic regression model with increased max_iter\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_scaled, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\\n\", classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62513989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.31      0.39       259\n",
      "           1       0.56      0.63      0.59       658\n",
      "           2       0.57      0.60      0.58       526\n",
      "\n",
      "    accuracy                           0.56      1443\n",
      "   macro avg       0.55      0.51      0.52      1443\n",
      "weighted avg       0.56      0.56      0.55      1443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train_scaled, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\\n\", classification_report(test_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f65e574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.31      0.41       259\n",
      "           1       0.56      0.66      0.60       658\n",
      "           2       0.60      0.61      0.60       526\n",
      "\n",
      "    accuracy                           0.58      1443\n",
      "   macro avg       0.58      0.53      0.54      1443\n",
      "weighted avg       0.58      0.58      0.57      1443\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/blankajarmoszko/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have a DataFrame df_cleaned with columns 'cleaned_text' and 'sentiment'\n",
    "# Replace 'your_dataset.csv' with the actual file or provide your own dataset\n",
    "# df_cleaned = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(df_cleaned['cleaned_text'], df_cleaned['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_train_data = [text.split(',') for text in train_data]\n",
    "tokenized_test_data = [text.split(',') for text in test_data]\n",
    "\n",
    "# Assuming you have already trained Word2Vec and created document vectors X_train and X_test\n",
    "# ... (as per your previous code)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train_scaled, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\\n\", classification_report(test_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4bcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
